# FINAL PROJECT - Multiple document format reconciler

## Project Overview

In my company we receive multiple files reporting the comission amount our sales team have received. These reports are generated in different formats (txt, CSV, PDF) and in different periodicity. Furthermore, the different files have different data columns and organization, as they are generated by different companies altogether.
I am encumbered with reconciling these reports which is a time consuming task, as I have to manually review each column, adjust it by the timeframe, in order to view our total production.

An AI web application that would review all documents and reconcile the information in a standard format would save us a lot of time, and hugely benefit our team.
--- 
## Use Case & Workflow

The program would benefit to make my workload faster. Our business is a lending company who partners with multiple banks, however the people who could potentially benefit from this program are any businesses who receive reports in different formats and periodicity.
---
## AI Features to Be Implemented
Prompt engineering - Use an enhanced prompt as a way to identify some of the columns (prompt example below) since the files being imported are somewhat consistent, there can be some expectation of what will be seen.

AI will be used for:
* Intelligent column mapping for lender-specific formats.
* Data normalization (e.g., standardizing date formats, currency values).
* Date matching for aligning sales dates to commission periods.
* Duplicate detection for previously imported entries.

### Few shot prompting 
As I need this to result in accurate values, an examples will be submitted along with the prompt so that the LLM has grounds for the source of truth. Financial terminology can be ambiguous and convoluted, so it's important we also set guardrails.
Structured outputs - Commission data needs to be consistently parsed into predictable fields. The output fields are 'date' (datetime),'product_name' (string),'gross_commission' (float),'net_commission' (float),'lender_name' (string) to be parsed into a webpage for easier display.

## Evaluation frameworks - 

Tracing - for logging the retrieved steps
A/B testing prompts to check if providing different examples make the model more accurate.
Chaining - Since my flow has multiple sequential steps (detect lender → parse → normalize → deduplicate → save to DB → update UI), chaining is necessary to ensures each step runs in order and passes structured data forward. This could be implemented without heavy AI orchestration tools, but still worth planning so future automation (like Gmail auto-import) is reliable.
Human in the Loop - This is important to have as I may have new lenders sending new file formats, low-confidence field mappings, or ambiguous date ranges should trigger a manual confirmation step.

Prevents bad data from entering your historical database

## Technical Approach

Open AI - LLM for reviewing the uploaded documentation and data identification
Pinecone - Vectorizing (for aproximation of column name definition)

Frontend:

Next.js app hosted on Vercel.
Dashboard for viewing commissions by date range and list of imported files.

Backend:

Node.js API handling file uploads and Gmail ingestion via MCP integration.
AI processing with OpenAI GPT-4o (or similar) for parsing and mapping.
File parsing with libraries for CSV/TXT, and pdfplumber for read-only PDFs.
Database:
PostgreSQL (Supabase) for storing normalized commission records and import metadata.
Vector database (e.g., Pinecone or pgvector) for storing column mapping embeddings.
Automation:
Gmail webhook triggers ingestion when new files are received, but also manual upload via online dashboard.
Role-based access control as this program is for internal use only.
---
## Example Prompts & Expected Outputs

PROMPT 
'''
You are an AI file parser. The following file contains commission report data from a lender.  
Your task is to identify relevant columns and output a JSON array following this schema:  
[ { sale_date, contract_id, customer_id, gross_commission, net_commission, lender_name } ]  

If you cannot confidently map a column, output `"unknown"` for that field and request clarification.
''' 

OUTPUT
'''
  {
    "date": "2025-07-12",
    "product_name": "personal credit",
    "gross_commission": 500.00,
    "net_commission": 450.00,
    "lender_name": "Banco X"
  }
'''
---
## Evaluation Strategy

* String comparison - Need to check if the input yields the expected result. Best choice given the repetitiveness nature of this task.
* Guardrails - as I am the end user, and there is authentication in place, I don't believe this is extensively necessary. 

---
## Observability Plan

Performance isn't as paramount when considering this application would run once a day, and perform a simple application. As I am the end user I don't consider this factor to be 
Error handling needs to be manually performed.

